# ollama_agent.py

import ollama

def generate_answer_from_annotation(file_path: str, user_question: str, model: str = "deepseek-coder"):
    """
    Use Ollama to generate a response based on an annotated file and a user's question.
    
    Args:
        file_path (str): Path to the uploaded annotated file (TSV, CSV, or VCF).
        user_question (str): The biological question from the user.
        model (str): Ollama model to use. Defaults to "deepseek-coder".

    Returns:
        str: Answer generated by the AI model.
    """
    try:
        # Read the annotated file content
        with open(file_path, 'r') as f:
            annotation_content = f.read()

        # Keep only relevant portion (e.g., top 100 lines) to fit in prompt context window
        lines = annotation_content.strip().split("\n")
        file_summary = "\n".join(lines[:100])  # You can increase if model has large context

        # Prompt template
        prompt = f"""
You are a genomics expert. Below is an excerpt from an annotated genetic variant file (VCF/TSV).

====================
ANNOTATION DATA:
{file_summary}
====================

QUESTION:
{user_question}

Answer clearly, using biological knowledge, and refer to genes, variants, and impact where applicable.
"""

        # Use Ollama to get the response
        response = ollama.chat(model=model, messages=[
            {"role": "user", "content": prompt}
        ])

        return response['message']['content']
    
    except Exception as e:
        return f"⚠️ Error processing file or calling Ollama: {str(e)}"
